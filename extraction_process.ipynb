{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b796e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary packages\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eeffe971",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\simon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Downloading NLTK data package\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7f59330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the URL file into a pandas DataFrame\n",
    "input_df = pd.read_excel('Input.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5f58b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing 11668.0: 'NoneType' object has no attribute 'get_text'\n",
      "Error processing 17671.4: 'NoneType' object has no attribute 'get_text'\n"
     ]
    }
   ],
   "source": [
    "# Scrapping all user agent\n",
    "error_urls = []\n",
    "\n",
    "for index, row in input_df.iterrows():\n",
    "    url = row['URL']\n",
    "    url_id = row['URL_ID']\n",
    "\n",
    "    # Using user agent\n",
    "    header = {'User-Agent': \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36\"}\n",
    "    try:\n",
    "        response = requests.get(url,headers=header)\n",
    "    except:\n",
    "        error_urls.append(url_id)\n",
    "        print(\"Error getting response of {}\".format(url_id))\n",
    "        continue\n",
    "        \n",
    "    try:\n",
    "        #Create soup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        #get title\n",
    "        title = soup.find('h1').get_text()\n",
    "        \n",
    "        #get the article text\n",
    "        article = \"\"\n",
    "        for p in soup.find_all('p'):\n",
    "            article += p.get_text()\n",
    "    except Exception as e:\n",
    "        error_urls.append(url_id)\n",
    "        print(f\"Error processing {url_id}: {str(e)}\")\n",
    "        continue\n",
    "    \n",
    "    # Write title and text to a file\n",
    "    file_name = 'C:\\\\Users\\\\simon\\\\Python Data\\\\Test Assign\\\\URL Text\\\\' + str(url_id) + '.txt'\n",
    "    with open(file_name, 'w',encoding='utf-8') as file:\n",
    "        file.write(title + '\\n' + article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84916d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# directories\n",
    "text_dir = \"C:\\\\Users\\\\simon\\\\Python Data\\\\Test Assign\\\\URL Text\"\n",
    "stopwords_dir = \"C:\\\\Users\\\\simon\\\\Python Data\\\\Test Assign\\\\StopWords\"\n",
    "sentiment_dir = \"C:\\\\Users\\\\simon\\\\Python Data\\\\Test Assign\\\\MasterDictionary\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adf4d8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all stop words into a set variable\n",
    "stop_words = set()\n",
    "for files in os.listdir(stopwords_dir):\n",
    "    with open(os.path.join(stopwords_dir, files), 'r', encoding='ISO-8859-1') as f:\n",
    "        stop_words.update(set(f.read().splitlines()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6bab80df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process and load all text files into a list\n",
    "docs = []\n",
    "for text_file in os.listdir(text_dir):\n",
    "    with open(os.path.join(text_dir, text_file), 'r',encoding='ISO-8859-1') as f:\n",
    "        text = f.read()\n",
    "        # Tokenize\n",
    "        words = word_tokenize(text)\n",
    "        # Remove stop words\n",
    "        filtered_text = [word for word in words if word.lower() not in stop_words]\n",
    "        # Append to list\n",
    "        docs.append(filtered_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c74fb4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read positive and negative files from directory\n",
    "pos = set()\n",
    "neg = set()\n",
    "for files in os.listdir(sentiment_dir):\n",
    "    if files == 'positive-words.txt':\n",
    "        with open(os.path.join(sentiment_dir, files), 'r', encoding='ISO-8859-1') as f:\n",
    "            pos.update(f.read().splitlines())\n",
    "    elif files == 'negative-words.txt':\n",
    "        with open(os.path.join(sentiment_dir, files), 'r', encoding='ISO-8859-1') as f:\n",
    "            neg.update(f.read().splitlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99249428",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting Derived Variables\n",
    "\n",
    "# calculate positive and negative score\n",
    "positive_words = [[word.lower() for word in doc if word.lower() in pos] for doc in docs]\n",
    "negative_words = [[word.lower() for word in doc if word.lower() in neg] for doc in docs]\n",
    "\n",
    "positive_score = [len(words) for words in positive_words]\n",
    "negative_score = [len(words) for words in negative_words]\n",
    "\n",
    "# calculate polarity and subjectivity score\n",
    "polarity_score = [(pos_score - neg_score) / (pos_score + neg_score + 0.000001) for pos_score, neg_score in zip(positive_score, negative_score)]\n",
    "subjectivity_score = [(pos_score + neg_score) / (len(doc) + 0.000001) for doc, pos_score, neg_score in zip(docs, positive_score, negative_score)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8601cddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fucntion to Calculate other NLP metrics\n",
    "\n",
    "def calculate_metric(file):\n",
    "    with open(os.path.join(text_dir, file), 'r',encoding='ISO-8859-1') as f:\n",
    "        text = f.read()\n",
    "        \n",
    "        # Remove punctuations and split into sentences\n",
    "        text = re.sub(r'[^\\w\\s.]', '', text)\n",
    "        sentences = text.split('.')\n",
    "        \n",
    "        num_sentences = len(sentences)\n",
    "        \n",
    "        # Total words\n",
    "        words = [word for word in text.split() if word.lower() not in stop_words]\n",
    "        num_words = len(words)\n",
    "        \n",
    "        # complex words\n",
    "        vowels = 'aeiou'\n",
    "        complex_words = [word for word in words if sum(1 for letter in word if letter.lower() in vowels) > 2]\n",
    "        \n",
    "        # syllable count\n",
    "        vowels = 'aeiou'\n",
    "        syllable_count = 0\n",
    "        syllable_words = []\n",
    "        \n",
    "        for word in words:\n",
    "            # Skip the entire word if it ends with \"es\" or \"ed\"\n",
    "            if word.endswith(('es', 'ed')):\n",
    "                continue  \n",
    "                \n",
    "            syllable_count_word = sum(1 for letter in word if letter.lower() in vowels)\n",
    "            \n",
    "            if syllable_count_word >= 1:\n",
    "                syllable_words.append(word)\n",
    "                syllable_count += syllable_count_word\n",
    "                \n",
    "        # sentence length\n",
    "        avg_sentence_len = num_words / num_sentences\n",
    "        \n",
    "        # syllable count per word\n",
    "        avg_syllable_word_count = syllable_count / len(syllable_words)\n",
    "        \n",
    "        # percent complex words\n",
    "        percent_complex_words = len(complex_words) / num_words\n",
    "        \n",
    "        # fog index\n",
    "        fog_index = 0.4 * (avg_sentence_len + percent_complex_words)\n",
    "        \n",
    "        # Word Count and Average Word Length\n",
    "        word_count = len(words)\n",
    "        average_word_length = sum(len(word) for word in words) / word_count\n",
    "        \n",
    "        return avg_sentence_len, percent_complex_words, fog_index, len(complex_words), avg_syllable_word_count, word_count, average_word_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "573162b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [calculate_metric(file) for file in os.listdir(text_dir)]\n",
    "avg_sentence_length, percentage_of_complex_words, fog_index, complex_word_count, avg_syllable_word_count, word_count, average_word_length = zip(*metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6444467e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since both Avg Sentence Length and Number of Words per sentence evaluate the same\n",
    "avg_words_per_sentence=avg_sentence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f477762d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count Personal Pronouns\n",
    "personal_pronouns = set([\"I\", \"we\", \"my\", \"ours\", \"us\"])  # Convert to a set for faster membership checks\n",
    "\n",
    "pronoun_counts = []\n",
    "\n",
    "for file in os.listdir(text_dir):\n",
    "    with open(os.path.join(text_dir, file), 'r', encoding='ISO-8859-1') as f:\n",
    "        text = f.read()\n",
    "        file_pronoun_count = sum(len(re.findall(r\"\\b\" + pronoun + r\"\\b\", text.lower()))for pronoun in personal_pronouns)\n",
    "        \n",
    "        # Exclude \"US\" as it is refering to the country\n",
    "        file_pronoun_count -= len(re.findall(r'\\bUS\\b', text))\n",
    "        \n",
    "        pronoun_counts.append(file_pronoun_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b96ff18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the output data structure\n",
    "output_df = pd.read_excel('Output Data Structure.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f81d6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows that got an error\n",
    "if error_urls:\n",
    "    # Filter rows to drop based on URL_IDs in error_urls\n",
    "    rows_to_drop = output_df[output_df['URL_ID'].isin(error_urls)].index\n",
    "    \n",
    "    # Drop the rows from the output_df dataframe\n",
    "    output_df.drop(rows_to_drop, axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4584999e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output variables\n",
    "variables = [\n",
    "    positive_score,\n",
    "    negative_score,\n",
    "    polarity_score,\n",
    "    subjectivity_score,\n",
    "    avg_sentence_length,  \n",
    "    percentage_of_complex_words,\n",
    "    fog_index,\n",
    "    avg_words_per_sentence,\n",
    "    complex_word_count,\n",
    "    word_count,\n",
    "    avg_syllable_word_count,\n",
    "    pronoun_counts,\n",
    "    average_word_length\n",
    "]\n",
    "\n",
    "# Write the values into DataFrame\n",
    "for i, var in enumerate(variables):\n",
    "    output_df[output_df.columns[i + 2]] = var\n",
    "    \n",
    "# Save the DataFrame to a CSV file\n",
    "output_df.to_csv('Output2.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c7c789",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
